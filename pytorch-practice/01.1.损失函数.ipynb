{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import math\n",
    "def configure_logging(level=logging.INFO):\n",
    "    format = '%(asctime)s %(filename)s:%(lineno)d %(levelname)s] %(message)s'\n",
    "    datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "    logging.basicConfig(level=level, format=format, datefmt=datefmt)\n",
    "configure_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "heading_collapsed": true
   },
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Embedding层可以训练后得到词向量，但是目前只有3种优化器可以使embedding层被学习到。\n",
    "\n",
    "Keep in mind that only a limited number of optimizers support sparse gradients: currently it’s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)\n",
    "\n",
    "另外from_pretrained方法可以直接加载预训练的词向量。\n",
    "\n",
    "With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:20:05.501480Z",
     "start_time": "2019-01-23T20:20:04.793Z"
    },
    "collapsed": false,
    "hidden": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1860, -0.7426, -0.8872,  0.1476, -0.2419],\n",
      "        [ 0.8223,  1.1087,  0.2378,  0.3572, -0.7053]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"], word_to_ix['world']])\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类的几种损失函数相关例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-15 17:50:56 <ipython-input-7-427e367640bc>:5 INFO] tensor([[ 1.3423,  0.2123, -0.1786,  1.0153, -0.0546],\n",
      "        [-0.4240, -0.8279,  0.3512,  0.3944,  0.6943],\n",
      "        [-0.3604, -0.2497, -1.0211, -0.2197,  0.8163]], requires_grad=True)\n",
      "2020-09-15 17:50:56 <ipython-input-7-427e367640bc>:6 INFO] tensor([0, 0, 1])\n",
      "2020-09-15 17:50:56 <ipython-input-7-427e367640bc>:16 INFO] 1.6583219766616821,1.6583219766616821\n",
      "/Users/wangming/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "2020-09-15 17:50:56 <ipython-input-7-427e367640bc>:21 INFO] tensor([[0.9203, 2.0503, 2.4412, 1.2473, 2.3172],\n",
      "        [2.2154, 2.6193, 1.4402, 1.3970, 1.0971],\n",
      "        [1.9499, 1.8393, 2.6106, 1.8092, 0.7733]], grad_fn=<NegBackward>)\n",
      "2020-09-15 17:50:56 <ipython-input-7-427e367640bc>:23 INFO] tensor([[1, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0]])\n",
      "2020-09-15 17:50:56 <ipython-input-7-427e367640bc>:25 INFO] tensor([[0.9203, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [2.2154, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.8393, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "2020-09-15 17:50:57 <ipython-input-7-427e367640bc>:27 INFO] tensor(1.6583, grad_fn=<MeanBackward0>)\n",
      "2020-09-15 17:50:57 <ipython-input-7-427e367640bc>:28 INFO] tensor(1.6583, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 模拟网络最后输出与目标值，[batchsize=3, num_labels=5]\n",
    "# 适用每个样本2分类或者多分类（但是分类是互斥的）\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "logging.info(input)\n",
    "logging.info(target)\n",
    "\"\"\"\n",
    "对比损失函数 CrossEntropyLoss和NLLLoss\n",
    "\"\"\"\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "loss2 = nn.NLLLoss()\n",
    "\n",
    "l1 = loss1(input, target)\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "l2 = loss2(m(input), target)\n",
    "logging.info('{},{}'.format(l1,l2))\n",
    "\"\"\"\n",
    "模拟NLLLoss的计算过程\n",
    "\"\"\"\n",
    "input_log_softmax = -torch.log(F.softmax(input))\n",
    "logging.info(input_log_softmax)\n",
    "target_one_hot = F.one_hot(target, num_classes=5)\n",
    "logging.info(target_one_hot)\n",
    "a = input_log_softmax * target_one_hot.float()\n",
    "logging.info(a)\n",
    "b = torch.mean(torch.sum(a, dim=1))\n",
    "logging.info(b)\n",
    "logging.info(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:7 INFO] tensor([[-0.0304,  0.9403, -0.4497],\n",
      "        [-1.9937, -0.8510,  0.8463],\n",
      "        [-0.2669,  1.4899, -0.4149]], requires_grad=True)\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:8 INFO] tensor([[0., 1., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 1.]])\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:12 INFO] tensor([[0.4924, 0.7192, 0.3894],\n",
      "        [0.1199, 0.2992, 0.6998],\n",
      "        [0.4337, 0.8161, 0.3977]], grad_fn=<SigmoidBackward>)\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:16 INFO] tensor(0.6935, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:19 INFO] tensor([[-0.6781, -0.3297, -0.9431],\n",
      "        [-0.1277, -0.3556, -0.3570],\n",
      "        [-0.8355, -1.6932, -0.9220]], grad_fn=<AddBackward0>)\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:22 INFO] tensor(0.6935, grad_fn=<NegBackward>)\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:24 INFO] BCELoss compute example\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:26 INFO] tensor(0.6935, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "2020-09-15 18:00:09 <ipython-input-8-cb80ddf15354>:27 INFO] tensor(0.6935, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "模拟BCELoss的计算过程，3个样本3个类别，target代表每个样本属于那些类别\n",
    "适用每个样本多标签分类（1个样本可能属于多个类别）\n",
    "\"\"\"\n",
    "input = torch.randn(3, 3, requires_grad=True)\n",
    "target = torch.FloatTensor([[0, 1, 1], [0, 0, 1], [1, 0, 1]])\n",
    "logging.info(input)\n",
    "logging.info(target)\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "a = m(input)\n",
    "logging.info(a)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "loss2 = nn.BCEWithLogitsLoss()\n",
    "logging.info(loss(m(input), target))\n",
    "\n",
    "b = target * torch.log(m(input)) + (1 - target) * torch.log(1 - m(input))\n",
    "logging.info(b)\n",
    "\n",
    "c = torch.mean(b)\n",
    "logging.info(-c)\n",
    "if torch.equal(loss(m(input), target), -c) is True:\n",
    "    logging.info('BCELoss compute example')\n",
    "\n",
    "logging.info(loss2(input, target))\n",
    "logging.info(loss(m(input), target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7109)\n",
      "tensor(0.7109)\n",
      "tensor(0.7109)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(3,3)\n",
    "target = torch.tensor([[0,1,0],[1,0,0],[1,1,1]], dtype=torch.float)\n",
    "m = torch.nn.Sigmoid()\n",
    "loss = torch.nn.BCELoss()\n",
    "print(loss(m(input), target))\n",
    "\n",
    "loss1 = torch.nn.BCEWithLogitsLoss()\n",
    "print(loss1(input, target))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "print(F.binary_cross_entropy_with_logits(input, target))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
