{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "heading_collapsed": true
   },
   "source": [
    "### Embedding\n",
    "Embedding层可以训练后得到词向量，但是目前只有3种优化器可以使embedding层被学习到。\n",
    "Keep in mind that only a limited number of optimizers support sparse gradients: currently it’s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)\n",
    "\n",
    "另外from_pretrained方法可以直接加载预训练的词向量。\n",
    "\n",
    "With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T20:20:05.501480Z",
     "start_time": "2019-01-23T20:20:04.793Z"
    },
    "collapsed": false,
    "hidden": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1860, -0.7426, -0.8872,  0.1476, -0.2419],\n",
      "        [ 0.8223,  1.1087,  0.2378,  0.3572, -0.7053]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"], word_to_ix['world']])\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类的几种损失函数相关例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:20 INFO] tensor([[ 0.5893,  1.2365, -0.4431,  0.5678,  0.2332],\n",
      "        [-1.0433, -1.4576, -0.4088, -2.3691,  0.0728],\n",
      "        [ 0.8016, -0.1755, -2.2973, -0.0646, -0.0961]], requires_grad=True)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:21 INFO] tensor([3, 3, 0])\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:33 INFO] CrossEntropyLoss = LogSoftmax + NLLLoss\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:38 INFO] tensor([[1.5985, 0.9513, 2.6309, 1.6199, 1.9545],\n",
      "        [1.9265, 2.3408, 1.2920, 3.2523, 0.8104],\n",
      "        [0.8108, 1.7878, 3.9096, 1.6769, 1.7084]], grad_fn=<NegBackward>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:40 INFO] tensor([[0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0]])\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:42 INFO] tensor([[0.0000, 0.0000, 0.0000, 1.6199, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 3.2523, 0.0000],\n",
      "        [0.8108, 0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:44 INFO] tensor(1.8943, grad_fn=<MeanBackward0>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:45 INFO] tensor(1.8943, grad_fn=<NllLossBackward>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:47 INFO] NLLLoss compute example\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:49 INFO] ============================================================\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:56 INFO] tensor([[-1.0510,  0.6603, -0.2581],\n",
      "        [-1.2912,  1.3740,  0.7274],\n",
      "        [-1.4352, -0.9550,  0.7829]], requires_grad=True)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:57 INFO] tensor([[0., 1., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 1.]])\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:61 INFO] tensor([[0.2590, 0.6593, 0.4358],\n",
      "        [0.2157, 0.7980, 0.6742],\n",
      "        [0.1923, 0.2779, 0.6863]], grad_fn=<SigmoidBackward>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:65 INFO] tensor(0.6816, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:68 INFO] tensor([[-0.2998, -0.4165, -0.8305],\n",
      "        [-0.2429, -1.5996, -0.3942],\n",
      "        [-1.6487, -0.3256, -0.3764]], grad_fn=<AddBackward0>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:71 INFO] tensor(0.6816, grad_fn=<NegBackward>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:73 INFO] BCELoss compute example\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:76 INFO] tensor(0.6816, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:77 INFO] tensor(0.6816, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "2019-10-07 16:41:17 <ipython-input-1-91a7ef90ce55>:83 INFO] BCELoss === BCEWithLogitsLoss\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import math\n",
    "def configure_logging(level=logging.INFO):\n",
    "    format = '%(asctime)s %(filename)s:%(lineno)d %(levelname)s] %(message)s'\n",
    "    datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "    logging.basicConfig(level=level, format=format, datefmt=datefmt)\n",
    "configure_logging()\n",
    "\n",
    "# 模拟网络最后输出与目标值，[batchsize=3, num_labels=5]\n",
    "# 适用每个样本2分类或者多分类（但是分类是互斥的）\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "logging.info(input)\n",
    "logging.info(target)\n",
    "\"\"\"\n",
    "对比损失函数 CrossEntropyLoss和NLLLoss\n",
    "\"\"\"\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "loss2 = nn.NLLLoss()\n",
    "\n",
    "l1 = loss1(input, target)\n",
    "l2 = loss2(m(input), target)\n",
    "if torch.equal(l1, l2) is True:\n",
    "    logging.info('CrossEntropyLoss = LogSoftmax + NLLLoss')\n",
    "\"\"\"\n",
    "模拟NLLLoss的计算过程\n",
    "\"\"\"\n",
    "input_log_softmax = -torch.log(F.softmax(input))\n",
    "logging.info(input_log_softmax)\n",
    "target_one_hot = F.one_hot(target, num_classes=5)\n",
    "logging.info(target_one_hot)\n",
    "a = input_log_softmax * target_one_hot.float()\n",
    "logging.info(a)\n",
    "b = torch.mean(torch.sum(a, dim=1))\n",
    "logging.info(b)\n",
    "logging.info(l2)\n",
    "if torch.equal(b, l2) is True:\n",
    "    logging.info('NLLLoss compute example')\n",
    "\n",
    "logging.info('=' * 60)\n",
    "\"\"\"\n",
    "模拟BCELoss的计算过程，3个样本3个类别，target代表每个样本属于那些类别\n",
    "适用每个样本多标签分类（1个样本可能属于多个类别）\n",
    "\"\"\"\n",
    "input = torch.randn(3, 3, requires_grad=True)\n",
    "target = torch.FloatTensor([[0, 1, 1], [0, 0, 1], [1, 0, 1]])\n",
    "logging.info(input)\n",
    "logging.info(target)\n",
    "\n",
    "m = nn.Sigmoid()\n",
    "a = m(input)\n",
    "logging.info(a)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "loss2 = nn.BCEWithLogitsLoss()\n",
    "logging.info(loss(m(input), target))\n",
    "\n",
    "b = target * torch.log(m(input)) + (1 - target) * torch.log(1 - m(input))\n",
    "logging.info(b)\n",
    "\n",
    "c = torch.mean(b)\n",
    "logging.info(-c)\n",
    "if torch.equal(loss(m(input), target), -c) is True:\n",
    "    logging.info('BCELoss compute example')\n",
    "\n",
    "logging.info(loss2(input, target))\n",
    "logging.info(loss(m(input), target))\n",
    "\n",
    "res1 = loss(m(input), target).item()\n",
    "res2 = loss2(input, target).item()\n",
    "\n",
    "if math.isclose(res1, res2, rel_tol=1e-5) is True:\n",
    "    logging.info('BCELoss === BCEWithLogitsLoss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 优化器相关例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.models import AlexNet\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "根据epoch来调整学习率的方式，可以结合不同的参数用不同的学习率方式\n",
    "\"\"\"\n",
    "model = AlexNet(num_classes=2)\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.05)\n",
    "\n",
    "# lr_scheduler.StepLR()\n",
    "# Assuming optimizer uses lr = 0.05 for all groups\n",
    "# lr = 0.05     if epoch < 30\n",
    "# lr = 0.005    if 30 <= epoch < 60\n",
    "# lr = 0.0005   if 60 <= epoch < 90\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "plt.figure()\n",
    "x = list(range(100))\n",
    "y = []\n",
    "for epoch in range(100):\n",
    "    # 更新了学习率，从而更新了optimizer中的学习率状态，继续后续的train过程\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_lr()\n",
    "    print(epoch, scheduler.get_lr()[0])\n",
    "    y.append(scheduler.get_lr()[0])\n",
    "\n",
    "plt.plot(x, y)\n",
    "\"\"\"\n",
    "# 网络中不同的参数用不同的学习率来学习的方式\n",
    "\"\"\"\n",
    "model = torchvision.models.resnet18()\n",
    "paras = dict(model.named_parameters())\n",
    "\n",
    "for k, v in paras.items():\n",
    "    print(k.ljust(30), str(v.shape).ljust(30), 'bias:', v.requires_grad)\n",
    "\n",
    "paras_new = []\n",
    "for k, v in paras.items():\n",
    "    if 'bias' in k:\n",
    "        paras_new += [{'params': [v], 'lr': 0.02, 'weight_decay': 0}]\n",
    "    else:\n",
    "        paras_new += [{'params': [v], 'lr': 0.01, 'weight_decay': 0.00004}]\n",
    "optimizer = torch.optim.SGD(paras_new, momentum=0.9)\n",
    "\n",
    "for p in optimizer.param_groups:\n",
    "    outputs = ''\n",
    "    for k, v in p.items():\n",
    "        if k is 'params':\n",
    "            outputs += (k + ': ' + str(v[0].shape).ljust(30) + ' ')\n",
    "        else:\n",
    "            outputs += (k + ': ' + str(v).ljust(10) + ' ')\n",
    "    print(outputs)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
