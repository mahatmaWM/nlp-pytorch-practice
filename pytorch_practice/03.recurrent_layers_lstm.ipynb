{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解lstm的内部细节，3个gate如何控制数据流向。\n",
    "\n",
    "https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent layers\n",
    "输入输出的tensor维度信息容易混淆，且还需注意batch_first这个参数，它影响tensor形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# input_size相当于input的dimension\n",
    "# hidden_size相当于是output的dimension，注意是否是双向的\n",
    "# num_layers是有几层lstm stack起来\n",
    "# batch_first是指明输出的batch这个维度是否在第一维\n",
    "rnn = nn.LSTM(10, 20, 2, batch_first=False)\n",
    "# (seq_len, batch, input_size)\n",
    "input = torch.randn(5, 3, 10)\n",
    "# (num_layers * num_directions, batch, hidden_size)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "# (num_layers * num_directions, batch, hidden_size)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "# (seq_len, batch, num_directions * hidden_size)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size相当于input的dimension\n",
    "# hidden_size相当于是output的dimension\n",
    "rnn = nn.LSTMCell(10, 20)\n",
    "# (batch, input_size)，这里的6相当于sentence的长度\n",
    "input = torch.randn(6, 3, 10)\n",
    "# (batch, input_size)\n",
    "hx = torch.randn(3, 20)\n",
    "# (batch, input_size)\n",
    "cx = torch.randn(3, 20)\n",
    "output = []\n",
    "# 处理sentence中的每一个token\n",
    "for i in range(6):\n",
    "    hx, cx = rnn(input[i], (hx, cx))\n",
    "    output.append(hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size相当于input的dimension\n",
    "# hidden_size相当于是output的dimension，注意是否是双向的\n",
    "# num_layers是有几层lstm stack起来\n",
    "# batch_first是指明输出的batch这个维度是否在第一维\n",
    "rnn = nn.RNN(10, 20, 2)\n",
    "# (seq_len, batch, input_size)\n",
    "input = torch.randn(5, 3, 10)\n",
    "# (num_layers * num_directions, batch, hidden_size)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "# output = (seq_len, batch, num_directions * hidden_size)\n",
    "# hn = (num_layers * num_directions, batch, hidden_size)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size相当于input的dimension\n",
    "# hidden_size相当于是output的dimension\n",
    "# nonlinearity – The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’\n",
    "rnn = nn.RNNCell(10, 20, nonlinearity='tanh')\n",
    "# (batch, input_size),这里的6相当于sentence的长度\n",
    "input = torch.randn(6, 3, 10)\n",
    "# (batch, hidden_size)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "# 处理sentence中的每一个token\n",
    "for i in range(6):\n",
    "    hx = rnn(input[i], hx)\n",
    "    output.append(hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size相当于input的dimension\n",
    "# hidden_size相当于是output的dimension，注意是否是双向的\n",
    "# num_layers是有几层lstm gru起来\n",
    "# batch_first是指明输出的batch这个维度是否在第一维\n",
    "rnn = nn.GRU(10, 20, 2)\n",
    "# (seq_len, batch, input_size)\n",
    "input = torch.randn(5, 3, 10)\n",
    "# (num_layers * num_directions, batch, hidden_size)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "# output=(seq_len, batch, num_directions * hidden_size)\n",
    "# hn=(num_layers * num_directions, batch, hidden_size)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size相当于input的dimension\n",
    "# hidden_size相当于是output的dimension\n",
    "rnn = nn.GRUCell(10, 20)\n",
    "# (batch, input_size),这里的6相当于sentence的长度\n",
    "input = torch.randn(6, 3, 10)\n",
    "# (batch, hidden_size)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "# 处理sentence中的每一个token\n",
    "for i in range(6):\n",
    "    hx = rnn(input[i], hx)\n",
    "    output.append(hx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
