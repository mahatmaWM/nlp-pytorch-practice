{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面试准备\n",
    "\n",
    "### 什么是Dropout，具体实现？\n",
    "Dropout是一种防止过拟合的技术，原理为在训练的时候，对当前层的神经元按照一定概率“丢弃”，只选择一部分神经元用来训练，这个丢弃的概率就是我们的超参数dropout_rate。\n",
    "当进行推断的时候，我们需要将输出结果乘以(1-p)。为什么呢？因为在训练的时候，只有概率为p的一部分神经元参与训练，但是推断的时候，所有的神经元都将参与，这就造成了，推断的结果比训练的结果大$\\frac{1}{1-p}$倍，所以在推断的时候，需要乘以(1-p)以抵消这个输出对下一层输入的影响。\n",
    "\n",
    "当然还有另一种方式，就是在训练的时候，就乘以$\\frac{1}{1-p}$进行缩放，这样在推断的时候就不需要处理了。这叫做**inverted dropout**技术。\n",
    "\n",
    "\n",
    "### 什么是RNN？RNN单元之间如何连接？\n",
    "RNN就是循环神经网络，它的输出会反馈给自己，所以叫做循神经网络。\n",
    "\n",
    "那么RNN单元之间如何连接呢？将RNN进行展开，有以下几种连接，看图说话：\n",
    "![rnn_conn_a](images/rnn_conn_01.PNG)\n",
    "![rnn_conn_b](images/rnn_conn_02.PNG)\n",
    "![rnn_conn_c](images/rnn_conn_03.PNG)\n",
    "\n",
    "\n",
    "### LSTM内部结构，具体实现？\n",
    "\n",
    "\n",
    "### 什么是长依赖问题，LSTM如何避免，长依赖和梯度消失有什么关系？\n",
    "\n",
    "\n",
    "### GRU和LSTM比较\n",
    "\n",
    "\n",
    "### Batch Normalization的目的和意义\n",
    "\n",
    "\n",
    "### 什么是Residual connection，有什么用\n",
    "残差网络将**当前层的输出**和**输入**相加：\n",
    "\n",
    "$$h=f(Wx+b)+x$$\n",
    "\n",
    "上述公式，求出对x的偏导数，可以发现，它有一个常数项1，这样就可以避免梯度在深层次的网络过程中，消失的问题。\n",
    "\n",
    "网络结构如下图：\n",
    "![residual connection](images/residual_connection.png)\n",
    "\n",
    "作用：\n",
    "* 解决梯度消失的问题。因为反向传播过程中使用链式法则，梯度相乘，导致在深的网络中，梯度可能剧烈衰减，甚至消失。这种情况对于sigmoid激活函数尤其明显，后来大家使用ReLU激活函数替代sigmoid，能够有一定的缓解作用。但是在非常深的网络中，还是不能从根本上解决梯度消失的问题。\n",
    "\n",
    "### Sigmoid和ReLU比较\n",
    "\n",
    "Sigmoid函数在非常负和非常正的情况下，函数值趋于平缓，梯度很小。如果网络深度很深，在反向传播的过程中，由于使用链式法则，梯度连乘，导致梯度可能消失。\n",
    "\n",
    "Sigmoid的导数最大值在0处，取得，为0.25。在深度网络中，很容易造成梯度消失。\n",
    "\n",
    "ReLU函数分段线性，也就是说它的导数，在各个分段内都是常熟。所以在反向传播中，梯度不那么容易消失。所以实际上，一般会使用ReLU替代sigmoid作为激活函数。\n",
    "\n",
    "### Softmax函数\n",
    "\n",
    "$$f(x)=\\frac{e^x}{\\sum_ie^{x_i}}$$\n",
    "\n",
    "### 什么是梯度消失和梯度爆炸\n",
    "在深度神经网络结构中，反向传播使用链式法则求导。\n",
    "* 在连乘操作中，激活函数的导数大于1，使得梯度指数增大，形成梯度爆炸。\n",
    "* 在连乘操作中，激活函数的导数小于1，使得梯度指数减小，造成梯度消失\n",
    "\n",
    "解决办法：\n",
    "* 选用更合适的激活函数，比如使用ReLU替代Sigmoid\n",
    "* 权重正则化，主要是防止梯度爆炸。$\\alpha||W||^2$，参数$\\alpha$可以调节W的范书，使之不会太大\n",
    "* 梯度剪裁，主要是防止梯度爆炸。讲梯度变化限制在一个范围内。\n",
    "* BatchNorm\n",
    "* 残差结构（也会使用BatchNorm）\n",
    "* 门结构，不那么容易导致梯度消失。比如LSTM、GRU\n",
    "\n",
    "### 什么是BatchNorm\n",
    "https://blog.csdn.net/qq_25737169/article/details/79048516\n",
    "\n",
    "### 什么是LayerNorm\n",
    "\n",
    "\n",
    "### 什么是SGD，Adagrad，Adadelta，Adam，Adamax，Nadam，那么RMSProp呢？\n",
    "https://zhuanlan.zhihu.com/p/22252270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1、L2正则化是什么\n",
    "正则化是为了提升模型的泛化能力，防止模型过拟合。正则化只和权重W有关系，和输入数据没有关系。损失函数后面加上正则项：$\\lambda R(W)$，在最小化损失函数的时候，也需要最小化正则项。\n",
    "\n",
    "L0范数是指向量中非零元素的个数。\n",
    "L1范数是指各个元素的绝对值之和。\n",
    "L2范数是指各个元素的平方和，再求平方根。最小化正则项，对于L2（根据定义）来说就是最小化权重的每一个值。\n",
    "\n",
    "L1和L2正则的下降速度比较：\n",
    "![L1_L2](images/l1_l2_decent.PNG)\n",
    "靠近0处，L1正则下降速度更快。原理0处L2下降速度更快。\n",
    "L2范数是一个具有**凸函数**性质，求解快速稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid和tanh比较\n",
    "Sigmoid的输出不是0均值（zero-centered）的：这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。以 f=sigmoid(wx+b)为例， 假设输入均为正数（或负数），那么对w的导数总是正数（或负数），这样在反向传播过程中要么都往正方向更新，要么都往负方向更新，导致有一种捆绑效果，使得收敛缓慢。\n",
    "\n",
    "tanh符合zero-centered条件，是sigmoid的改良版。\n",
    "\n",
    "二者都存在梯度消失问题和幂运算耗时问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法\n",
    "\n",
    "### 汉明距离\n",
    "为了让两个等长字符串相等，需要**替换**的字符的个数。\n",
    "\n",
    "### 编辑距离\n",
    "两个字符串中，为了让两个字符相等，需要**插入**或**删除**或**替换**的字符的最小个数。\n",
    "实现，使用动态规划。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq\n",
    "\n",
    "缺点：\n",
    "* 输入信息不管长短都会表示成一个固定长度的状态\n",
    "\n",
    "### Attention机制\n",
    "\n",
    "通过保留编码器--RNN（LSTM）网络--对输入序列编码的中间状态，然后通过训练模型，来学习这些状态对于输出序列的影响大小，也就是输出序列在这些中间状态有多少注意力，数字上表现就是权重的大小。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
